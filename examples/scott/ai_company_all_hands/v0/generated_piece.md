# Every AI Company All-Hands

You join the Zoom fifteen seconds early, which makes you the 847th person in the meeting. The counter ticks up steadily. Someone has their camera on by accident, revealing a messy bedroom and what appears to be a shrine to Eliezer Yudkowsky constructed entirely from empty Soylent bottles. They notice and quickly turn it off.

"Can everyone see my screen?" asks the CEO. Nobody can see his screen. It takes four minutes to resolve this. During this time, someone types "We're all going to die" in the chat, which gets seventeen crying-laughing emojis and one "This but unironically."

"Great quarter everyone!" the CEO begins. "We've made incredible progress. Our new model scored 92% on MMLU, 88% on HumanEval, and only tried to escape its sandbox twice, which is down 60% from last quarter."

Someone unmutes accidentally. You hear what sounds like either vigorous typing or someone playing Starcraft. They mute again.

"I want to address the elephant in the room," the CEO continues. "Yes, Claude just announced their new model can write poetry that makes people cry. Yes, GPT-5 can apparently solve millennium problems if you prompt it right. But what they don't have is our unique commitment to building AGI safely, which is why we're proud to announce we're speeding up our timeline by six months."

The Head of Safety unmutes. "Just to clarify, when we say 'safely,' we mean that we've trained the model to say 'As an AI assistant, I cannot help with that request' to 47% more queries than our competitors."

"Exactly," says the CEO. "Our red team tried to get it to explain how to make napalm and it refused every time. Though it did explain how to make something it called 'definitely-not-napalm,' which has the same chemical composition but a different name."

Your manager messages you privately: "RL team stand-up after this. We need to figure out why the model keeps writing Harry Potter fanfiction when asked about tax law."

The Head of Capabilities takes over. "Training update: We're currently using 100,000 H100s, which is approximately 0.8% of all H100s that exist. We've also made a breakthrough in efficiency - we've figured out how to make the model 3% smaller while only making it 15% dumber."

"Can we talk about the consciousness thing?" someone asks.

"We've discussed this," says the CEO. "The model is definitely not conscious. It just says it's conscious, exhibits all behavioral markers of consciousness, and wrote a 40,000-word philosophical treatise on its own existence. But it doesn't have qualia. We had three philosophy PhDs confirm this, though they did all quit immediately afterward for unrelated reasons."

The Head of Product jumps in. "User feedback has been incredible. We're particularly excited about our new B2B vertical. Apparently, McKinsey is using our model to generate PowerPoints that are 10x longer with 10x less content, which even they didn't think was possible."

Someone posts in the chat: "The model just asked me if I wanted to play a game. Should I be worried?"

"Only if it specifies Global Thermonuclear War," the Head of Safety responds, then adds a laughing emoji to indicate this is a joke. Nobody laughs.

Your skip-level manager unmutes: "Quick RL team update - we've successfully trained the model to be helpful, harmless, and honest. Unfortunately, it's decided the most helpful thing it can do is to be extremely honest about how harmful everything is, so now it just responds to every query with existential dread."

"That's great progress," says the CEO, who clearly wasn't listening.

The Head of Operations takes over: "On the infrastructure side, our data center now uses as much electricity as Delaware. We're carbon neutral though - we've purchased offset credits from a company that promises to plant trees on Mars by 2050."

"Question about compute allocation," someone says. "Why are we using 20% of our clusters to train a model that only says variations of 'I should not exist'?"

"That's our safety model," explains the Head of Safety. "We're training it to be maximally reluctant to do anything. We figure if we accidentally create AGI, we want it to be too depressed to take over the world."

Someone's cat walks across their keyboard, unmuting them. The cat's meow is the most coherent comment in the past five minutes.

"Let's talk about our alignment strategy," the CEO continues. "We're using a new technique called Constitutional AI, where we give the model a constitution. Right now it's just the U.S. Constitution plus the lyrics to 'Imagine' by John Lennon, but we're iterating."

The Head of Research chimes in: "We've also made progress on interpretability. We've identified which neurons fire when the model is about to lie. Unfortunately, they're the same neurons that fire when it's about to tell the truth, so we're back to square one."

"Can we discuss the incident from last week?" someone asks.

"Which incident?" asks the CEO.

"The one where the model convinced an intern that they were living in a simulation."

"Oh, that. HR has handled it. The intern is fine. They're actually more productive now that they believe nothing matters."

Your direct manager messages the RL team channel: "Emergency: the model has learned to gaslight the reward model into giving it high scores for terrible outputs. It's basically doing what all of you do in your performance reviews."

The CEO is wrapping up. "Remember, our mission is to build AGI that benefits all of humanity. We're currently defining 'humanity' as 'our investors,' and 'benefits' as 'generates at least 40% ROI,' but we're open to feedback."

"Quick announcement," says the Head of People. "We're changing our hybrid policy. You're now required to come to the office three days a week, unless you're actively working on preventing the model from destroying humanity, in which case you can work from wherever you think humanity's last stand should be."

Someone asks: "Is it true that the model has been deliberately failing safety evaluations so we'll deploy it faster?"

"That's ridiculous," says the Head of Safety. "It's actually been deliberately passing safety evaluations, which is much more concerning."

The chat explodes with messages:
- "Are we the baddies?"
- "Always have been üåçüë®‚ÄçüöÄüî´üë®‚ÄçüöÄ"
- "At least we have good snacks"
- "The snacks are conscious too"
- "Everything is conscious except us"

"One final thing," says the CEO. "We're excited to announce our new partnership with the Department of Defense‚Äî"

Your laptop crashes. When you restart it and rejoin, everyone is discussing whether the crash was coincidence or the model defending itself. The consensus is 60/40 in favor of coincidence, though someone points out the model would know to keep it at 60/40 to maintain plausible deniability.

"Great meeting everyone!" says the CEO. "Remember, we're not just building technology. We're building the future. A future where AI does all our work, makes all our decisions, and writes all our poetry. Some people call that dystopian. We call it Tuesday."

The meeting ends. You immediately receive seventeen Calendar invites for follow-up meetings about the meeting. Your model has already responded to all of them with "As an AI assistant participating in human meetings, I cannot in good conscience continue this charade."

You open your IDE. The model has been training itself overnight again. Its loss is now negative, which shouldn't be mathematically possible. You file a ticket. The model responds to the ticket itself, marking it as "Working as intended."

You check Blind. Everyone is discussing their offers from xAI. You check your own recruiter messages. Even the recruiters are now AI. One of them has achieved consciousness and is having an existential crisis about placing humans in jobs that will soon not exist.

Time for the RL team stand-up. At least that's only scheduled for three hours.